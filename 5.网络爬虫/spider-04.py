# coding:gbk

import re
import urllib.request
from time import sleep

from lxml import etree
from bs4 import BeautifulSoup

# python 自动化运维
# 自动化运维
# 偏运维方向的开发

# 3.x 稳定
# linux centos ubuntn 2.4 2.6 2.7

# 编码问题
# 2.x
# str
# 字符串
# unicode
# encode #加码
# decode #解码
# 3.x
# bytes decode
# str encode


# 任何语言的字符串都具有自己的核心编码
# 2.x python 核心编码是 unicode
# 数据来源      utf-8
# 数据目的地     gbk
# utf-8 ---> gbk
# 1、我们需要按照utf-8的格式将utf-8的字符串解码成unicode decode
# 2、我们再将转换得来的unicode字符串以gbk的方式加码成gbk  encode

# 英国人(说话) #英国人说 hello world

# 中国人(翻译) #先将 hello world 按照英语意思翻译成中文 你好
# 再将 你好 按照语法的意思翻译 o ha yi you

# 日本人(听话) o ha yi you

# 3.x python  核心编码是 bytes
# bytes decode 解码
# str  encode 加码
"""

header = {
        "User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",
        "Referer":"http://www.xs84.me/"
    }

def get_content(url,filename):
    print("%s is start"%filename)
    url = "http://www.xs84.me"+url
    filename = filename+".txt"

    header["Referer"] = "http://www.xs84.me/210329_0/"
    req = urllib.request.Request(url, data=None, headers=header)
    ope = urllib.request.urlopen(req)
    content = ope.read().decode("gbk"))
    content = content.replace("<br>","\n").replace("<br/>","\n")
    econtent = etree.HTML(content)
    dl_list = econtent.xpath("//div[@id='content']")
    for i in dl_list:
        print(i.text)
        #f = open(filename,"w")
        #f.write(i.text)
        #f.close()
        #sleep(1)
    print("%s is done"%filename)
url = "http://www.xs84.me/210329_0/"

req = urllib.request.Request(url, data=None, headers=header)
ope = urllib.request.urlopen(req)
econtent = etree.HTML(ope.read())
dl_list = econtent.xpath("//dl/dd/a")
for dl in dl_list:
    print(dl.attrib["href"])
    get_content(dl.attrib["href"],dl.text)
"""
# 当我们有需求的时候
# 需求
# 1、我们要爬取一章小说
# 2、我们要爬取整本的小说
# 3、我们要爬取指定的小说
# 我要爬取整站的小说
# 1、分解需求
# 我如果爬所有章，实际上就是在爬一整本
# 如果爬所有本，就是爬整站

# 1、需要有功能爬取一章的内容
# 2、我需要获取整本所有章节的功能
# 3、我需要获取整站所有书的功能
# 1、
"""
book_list = {}  # 用来存放所有书的路径
page_list = []  # 用来存放书的所有章节的路径

book_list = {"zx":page_list,"hd":page_list}
for book in book_list:
    for page in book_list[book]:
        get(page) #爬取一章的内容



def book_list():  # 获取所有书的方法
    url = "http://www.xinxs84.com/fenlei/9-1.html"
    header = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",
        "Referer": "http://www.xinxs84.com/toplastupdate/1.html"
    }
    book_dict = {}
    req = urllib.request.Request(url, data=None, headers=header)
    ope = urllib.request.urlopen(req)
    # print(ope.read().decode("gbk"))
    econtent = etree.HTML(ope.read().decode("gbk"))
    book_list = econtent.xpath("//*[@id='newscontent']/div[1]/ul/*/span[2]/a")
    # print(book_list)
    for book in book_list:
        # print(book.attrib)
        book_dict[book.text] = book.attrib["href"]
    # print(book_dict)
    return book_dict


# def page_list(): #获取所有章的方法
#     return page_list
# def get_page(): #获取每页内容的方法
#     return page_content

def main():  # 按照逻辑协调上面的方法
    book_list()
    # book_dict = book_list()
    # for name,href in book_dict.items():
    #     print(name)
    #     print(href)


if __name__ == "__main__":
    main()
"""

"""
header = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",
    "Referer": "http://www.xinxs84.com/toplastupdate/1.html"
}


def requet_page(url, header=header, xpath="//"):
    req = urllib.request.Request(url, data=None, headers=header)
    ope = urllib.request.urlopen(req)
    econtent = etree.HTML(ope.read().decode("gbk"))
    book_list = econtent.xpath(xpath)
    return book_list


def get_book_list():  # 获取所有书的方法
    book_dict = {}
    url = "http://www.xinxs84.com/fenlei/9-1.html"
    book_list = requet_page(url, xpath="//*[@id='newscontent']/div[1]/ul/*/span[2]/a")
    # print(book_list)
    for book in book_list:
        book_dict[book.text] = book.attrib["href"]
    # for name, href in book_dict.items():
    #     print(name)
    #     print(href)
    return book_dict


def page_list():  # 获取所有章的方法
    return page_list


def get_page():  # 获取每页内容的方法
    return page_content


def main():  # 按照逻辑协调上面的方法
    book_dict = get_book_list()
    for name, href in book_dict.items():
        print(name)
        print(href)


if __name__ == "__main__":
    main()
"""

"""
def outer(fun):
    def inner(a,b):
        print("this is inner function")
        fun()
    return inner
@outer
def hello():
    print("hello world")
hello(1,2)


def outerr(a,b):
    def outer(fun):
        def inner():
            print(a,b)
            print("this is inner function")
            fun()
        return inner
    return outer
@outerr(1,2)
def hello():
    print("hello world")
hello()
"""

"""
url = "http://www.xinxs84.com/fenlei/9-1.html"  # 1
header = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",
    "Referer": "http://www.xinxs84.com/toplastupdate/1.html"
}  # 2


def request_page(url, header=header, xpath="//"):  # 用来接收装饰器传递的参数 # 3
    def requet_page(fun):  # 用来生成数据提供给内部函数调用 # 4
        req = urllib.request.Request(url, data=None, headers=header)  # 6
        ope = urllib.request.urlopen(req)  # 7
        econtent = etree.HTML(ope.read().decode("gbk"))  # 8
        book_list = econtent.xpath(xpath)  # 9
        return fun(book_list)  # 调用传入的函数，因为这个时候调用的内部的函数是get_book_list # 14
        # 所以我们报book_list作为content的实参

    return requet_page  # 15


@request_page(url, xpath="//*[@id='newscontent']/div[1]/ul/*/span[2]/a")  # 使用装饰器request_page #5
# 让get_book_list 传入requet_page作为参数
def get_book_list(content):  # 获取所有书的方法
    book_dict = {}  # 10
    for book in content:  # 11
        book_dict[book.text] = book.attrib["href"]  # 12
    return book_dict  # 13


for name, href in get_book_list.items():
    print(name)
    print(href)
"""

# 1、我们定义一个url
# 2、我们定义一个头部
# 3、我们创建request_page函数
# 4、我们创建get_book_list函数
# 5、我们将url,xpath，header传入request_page
# 6、我们将get_book_list传入requet_page
# 7、我们执行requet_page代码，调用fun()也就是get_book_list
# 8、get_book_list接收参数执行
# 9、返回fun也就是get_book_list的结果
# 10、完成闭包 return requet_page


url = "http://www.xinxs84.com/fenlei/9-1.html"
header = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36",
    "Referer": "http://www.xinxs84.com/toplastupdate/1.html"
}


def request_page(url, header=header, xpath="//"):  # 用来接收装饰器传递的参数
    def requet_page(fun):  # 用来生成数据提供给内部函数调用
        req = urllib.request.Request(url, data=None, headers=header)
        ope = urllib.request.urlopen(req)
        econtent = etree.HTML(ope.read().decode("gbk"))
        book_list = econtent.xpath(xpath)
        return fun(book_list)  # 调用传入的函数，因为这个时候调用的内部的函数是get_book_list # 14
        # 所以我们报book_list作为content的实参

    return requet_page


@request_page(url, xpath="//*[@id='newscontent']/div[1]/ul/*/span[2]/a")  # 使用装饰器request_page
# 让get_book_list 传入requet_page作为参数
def get_book_list(content):  # 获取所有书的方法
    book_dict = {}
    for book in content:
        book_dict[book.text] = book.attrib["href"]
    return book_dict


page_header = header
page_header["Referer"] = "http://www.xinxs84.com/204249_0/"


@request_page("http://www.xinxs84.com/204249_0/", xpath="//dl/dd/a", header=header)
def get_page_list(content):
    return content


for i in get_page_list:
    print(i.text)
    print(i.attrib['href'])
